{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 79170 sentences.\n",
      "Parsed 79170 sentences.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "with open('data/reddit-comments-2015-08.csv', 'rb' ) as f:\n",
    "    reader = csv.reader(f, skipinitialspace = True)\n",
    "    reader.next()\n",
    "    # split doc into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(doc[0].decode('utf-8').lower()) for doc in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, sent, sentence_end_token) for sent in sentences]\n",
    "    print \"Parsed {} sentences.\".format(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 65751 unique words tokens\n",
      "Total of 65751 unique words tokens\n"
     ]
    }
   ],
   "source": [
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "token_freq = nltk.FreqDist(itertools.chain(*token_sentences))\n",
    "print \"Total of {} unique words tokens\".format(len(token_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a vocabulary sized restricted to 8000\n",
      "The least frequent word is 'devoted' with 10 counts in the reddit comments\n",
      "With a vocabulary sized restricted to 8000\n",
      "The least frequent word is 'devoted' with 10 counts in the reddit comments\n"
     ]
    }
   ],
   "source": [
    "vocab = token_freq.most_common(vocabulary_size - 1)\n",
    "index2vocab = [word[0] for word in vocab]\n",
    "index2vocab.append(unknown_token)\n",
    "word2index = {word: i for i, word in enumerate(index2vocab)}\n",
    "print \"With a vocabulary sized restricted to {}\".format(len(word2index))\n",
    "print \"The least frequent word is '{}' with {} counts in the reddit comments\".format(vocab[-1][0],vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START i put in the rules at a ranking site and noticed that top qbs had 300 points more than the top rb/wr. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'put', u'in', u'the', u'rules', u'at', u'a', u'ranking', u'site', u'and', u'noticed', u'that', u'top', u'qbs', u'had', u'300', u'points', u'more', u'than', u'the', u'top', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END']'\n",
      "\n",
      "Example sentence: 'SENTENCE_START i put in the rules at a ranking site and noticed that top qbs had 300 points more than the top rb/wr. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'put', u'in', u'the', u'rules', u'at', u'a', u'ranking', u'site', u'and', u'noticed', u'that', u'top', u'qbs', u'had', u'300', u'points', u'more', u'than', u'the', u'top', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(token_sentences):\n",
    "    token_sentences[i] = [word if word in word2index else unknown_token for word in sent]\n",
    "\n",
    "print \"\\nExample sentence: '{}'\".format(sentences[4])\n",
    "print \"\\nExample sentence after Pre-processing: '{}'\".format(token_sentences[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word2index[word] for word in sent[:-1]] for sent in token_sentences])\n",
    "y_train = np.asarray([[word2index[word] for word in sent[1:]] for sent in token_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ [0, 6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2],\n",
       "       [0, 11, 17, 7, 3114, 6036, 7999, 7999, 6036, 2]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ [0, 6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2],\n",
       "       [0, 11, 17, 7, 3114, 6036, 7999, 7999, 6036, 2]], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START i joined a new league this year and they have different scoring rules than i 'm used to . \n",
      "[0, 6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2]\n",
      "y:\n",
      "i joined a new league this year and they have different scoring rules than i 'm used to . SENTENCE_END \n",
      "[6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2, 1]\n",
      "x:\n",
      "SENTENCE_START i joined a new league this year and they have different scoring rules than i 'm used to . \n",
      "[0, 6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2]\n",
      "y:\n",
      "i joined a new league this year and they have different scoring rules than i 'm used to . SENTENCE_END \n",
      "[6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print \"x:\\n{} \\n{}\".format(\" \".join([index2vocab[index] for index in X_train[0]]), X_train[0])\n",
    "print \"y:\\n{} \\n{}\".format(\" \".join([index2vocab[index] for index in y_train[0]]), y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        xt = np.exp(x - np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # one sentence at the time\n",
    "        # total number of time steps\n",
    "        T = len(x)\n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        for t in np.arange(T):\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = self.softmax(self.V.dot(s[t]))     \n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # for each sentence\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            correct_predicitons = o[np.arange(len(y[i])), y[i]]\n",
    "            L += -1 * np.sum(np.log(correct_predicitons))\n",
    "        return L\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = sum(len(y_i) for y_i in y)\n",
    "        return self.total_loss(x, y) / N\n",
    "    \n",
    "    def backpropagation_tt(self, x, y):\n",
    "        T = len(y)\n",
    "        # forwardpropagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # save the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        \n",
    "        # For each output backwards\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - s[t]**2)\n",
    "            \n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                #print \"Backpropagation step t= {} bptt step= {} \".format(t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1]) \n",
    "                dLdU[:, x[bptt_step]] += delta_t\n",
    "                # update delta_t\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step - 1]**2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dLdU, dLdV, dLdW = self.backpropagation_tt(x, y)\n",
    "        \n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "        \n",
    "    def train_sgd(self, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        \n",
    "        losses = []\n",
    "        examples_seen = 0\n",
    "        \n",
    "        for epoch in xrange(nepoch):\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = self.loss(X_train, y_train)\n",
    "                losses.append((examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print \"{}: Loss after num_examples_seen={} epoch={}: {}\".format(time, examples_seen, epoch, loss)\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate *= 0.5\n",
    "                    print \"Setting learning rate to {}\".format(learning_rate)\n",
    "                sys.stdout.flush()\n",
    "                        \n",
    "            for i in range(len(y_train)):\n",
    "                self.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "                examples_seen += 1\n",
    "                \n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation.\n",
    "        bptt_gradients = model.backpropagation_tt(x, y)\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = model.total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = model.total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/\n",
    "                                (np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is to large fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                    print \"+h Loss: %f\" % gradplus\n",
    "                    print \"-h Loss: %f\" % gradminus\n",
    "                    print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                    print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                    print \"Relative Error: %f\" % relative_error\n",
    "                    return \n",
    "                it.iternext()\n",
    "            print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488\n",
      "   0.00012508]\n",
      " [ 0.00012536  0.00012582  0.00012436 ...,  0.00012482  0.00012456\n",
      "   0.00012451]\n",
      " [ 0.00012387  0.0001252   0.00012474 ...,  0.00012559  0.00012588\n",
      "   0.00012551]\n",
      " ..., \n",
      " [ 0.00012414  0.00012455  0.0001252  ...,  0.00012487  0.00012494\n",
      "   0.0001263 ]\n",
      " [ 0.0001252   0.00012393  0.00012509 ...,  0.00012407  0.00012578\n",
      "   0.00012502]\n",
      " [ 0.00012472  0.0001253   0.00012487 ...,  0.00012463  0.00012536\n",
      "   0.00012665]]\n",
      "(45, 8000)\n",
      "[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488\n",
      "   0.00012508]\n",
      " [ 0.00012536  0.00012582  0.00012436 ...,  0.00012482  0.00012456\n",
      "   0.00012451]\n",
      " [ 0.00012387  0.0001252   0.00012474 ...,  0.00012559  0.00012588\n",
      "   0.00012551]\n",
      " ..., \n",
      " [ 0.00012414  0.00012455  0.0001252  ...,  0.00012487  0.00012494\n",
      "   0.0001263 ]\n",
      " [ 0.0001252   0.00012393  0.00012509 ...,  0.00012407  0.00012578\n",
      "   0.00012502]\n",
      " [ 0.00012472  0.0001253   0.00012487 ...,  0.00012463  0.00012536\n",
      "   0.00012665]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "RNN_model = RNN(vocabulary_size)\n",
    "o, s = RNN_model.forward_propagation(X_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481  238 2539\n",
      "   21 6548  261 1780 2005 1810 5376 4146  477 7051 4832 4991  897 3485   21\n",
      " 7291 2007 6006  760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575]\n",
      "students shortly museum ruining background hunt madden wr chicken immoral hadith lighter rude questions achieve but sells making fill arguing purchase grows feat head lube winners downside states steal but researchers christian utilize fire domain resolution 10-15 genuinely magical worship в branches memes node preferred\n",
      "(45,)\n",
      "[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481  238 2539\n",
      "   21 6548  261 1780 2005 1810 5376 4146  477 7051 4832 4991  897 3485   21\n",
      " 7291 2007 6006  760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575]\n",
      "students shortly museum ruining background hunt madden wr chicken immoral hadith lighter rude questions achieve but sells making fill arguing purchase grows feat head lube winners downside states steal but researchers christian utilize fire domain resolution 10-15 genuinely magical worship в branches memes node preferred\n"
     ]
    }
   ],
   "source": [
    "pred = RNN_model.predict(X_train[10])\n",
    "print pred.shape\n",
    "print pred\n",
    "print \" \".join([index2vocab[index] for index in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print \"Expected Loss for random predictions: \", np.log(vocabulary_size)\n",
    "# print \"Current Loss: \",RNN_model.loss(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backpropagation step t= 3 bptt step= 3 \n",
      "Backpropagation step t= 3 bptt step= 2 \n",
      "Backpropagation step t= 3 bptt step= 1 \n",
      "Backpropagation step t= 3 bptt step= 0 \n",
      "Backpropagation step t= 2 bptt step= 2 \n",
      "Backpropagation step t= 2 bptt step= 1 \n",
      "Backpropagation step t= 2 bptt step= 0 \n",
      "Backpropagation step t= 1 bptt step= 1 \n",
      "Backpropagation step t= 1 bptt step= 0 \n",
      "Backpropagation step t= 0 bptt step= 0 \n",
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.Backpropagation step t= 3 bptt step= 3 \n",
      "Backpropagation step t= 3 bptt step= 2 \n",
      "Backpropagation step t= 3 bptt step= 1 \n",
      "Backpropagation step t= 3 bptt step= 0 \n",
      "Backpropagation step t= 2 bptt step= 2 \n",
      "Backpropagation step t= 2 bptt step= 1 \n",
      "Backpropagation step t= 2 bptt step= 0 \n",
      "Backpropagation step t= 1 bptt step= 1 \n",
      "Backpropagation step t= 1 bptt step= 0 \n",
      "Backpropagation step t= 0 bptt step= 0 \n",
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n",
      "\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marvinbertin/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:125: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/marvinbertin/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:125: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNN(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-11-01 23:13:56: Loss after num_examples_seen=0 epoch=0: 8.9874245324\n",
      "2015-11-01 23:13:56: Loss after num_examples_seen=0 epoch=0: 8.9874245324\n",
      "2015-11-01 23:14:09: Loss after num_examples_seen=100 epoch=1: 8.97627030828\n",
      "2015-11-01 23:14:09: Loss after num_examples_seen=100 epoch=1: 8.97627030828\n",
      "2015-11-01 23:14:23: Loss after num_examples_seen=200 epoch=2: 8.96021174257\n",
      "2015-11-01 23:14:23: Loss after num_examples_seen=200 epoch=2: 8.96021174257\n",
      "2015-11-01 23:14:37: Loss after num_examples_seen=300 epoch=3: 8.93042979657\n",
      "2015-11-01 23:14:37: Loss after num_examples_seen=300 epoch=3: 8.93042979657\n",
      "2015-11-01 23:14:49: Loss after num_examples_seen=400 epoch=4: 8.86226430002\n",
      "2015-11-01 23:14:49: Loss after num_examples_seen=400 epoch=4: 8.86226430002\n",
      "2015-11-01 23:15:02: Loss after num_examples_seen=500 epoch=5: 6.91356992751\n",
      "2015-11-01 23:15:02: Loss after num_examples_seen=500 epoch=5: 6.91356992751\n",
      "2015-11-01 23:15:14: Loss after num_examples_seen=600 epoch=6: 6.30249269658\n",
      "2015-11-01 23:15:14: Loss after num_examples_seen=600 epoch=6: 6.30249269658\n",
      "2015-11-01 23:15:26: Loss after num_examples_seen=700 epoch=7: 6.01499526592\n",
      "2015-11-01 23:15:26: Loss after num_examples_seen=700 epoch=7: 6.01499526592\n",
      "2015-11-01 23:15:39: Loss after num_examples_seen=800 epoch=8: 5.83387729521\n",
      "2015-11-01 23:15:39: Loss after num_examples_seen=800 epoch=8: 5.83387729521\n",
      "2015-11-01 23:15:51: Loss after num_examples_seen=900 epoch=9: 5.71071844488\n",
      "2015-11-01 23:15:51: Loss after num_examples_seen=900 epoch=9: 5.71071844488\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNN(vocabulary_size)\n",
    "losses = model.train_sgd(X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
