{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 79170 sentences.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "with open('data/reddit-comments-2015-08.csv', 'rb' ) as f:\n",
    "    reader = csv.reader(f, skipinitialspace = True)\n",
    "    reader.next()\n",
    "    # split doc into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(doc[0].decode('utf-8').lower()) for doc in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, sent, sentence_end_token) for sent in sentences]\n",
    "    print \"Parsed {} sentences.\".format(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 65751 unique words tokens\n"
     ]
    }
   ],
   "source": [
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "token_freq = nltk.FreqDist(itertools.chain(*token_sentences))\n",
    "print \"Total of {} unique words tokens\".format(len(token_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a vocabulary sized restricted to 8000\n",
      "The least frequent word is 'devoted' with 10 counts in the reddit comments\n"
     ]
    }
   ],
   "source": [
    "vocab = token_freq.most_common(vocabulary_size - 1)\n",
    "index2vocab = [word[0] for word in vocab]\n",
    "index2vocab.append(unknown_token)\n",
    "word2index = {word: i for i, word in enumerate(index2vocab)}\n",
    "print \"With a vocabulary sized restricted to {}\".format(len(word2index))\n",
    "print \"The least frequent word is '{}' with {} counts in the reddit comments\".format(vocab[-1][0],vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START i put in the rules at a ranking site and noticed that top qbs had 300 points more than the top rb/wr. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'put', u'in', u'the', u'rules', u'at', u'a', u'ranking', u'site', u'and', u'noticed', u'that', u'top', u'qbs', u'had', u'300', u'points', u'more', u'than', u'the', u'top', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(token_sentences):\n",
    "    token_sentences[i] = [word if word in word2index else unknown_token for word in sent]\n",
    "\n",
    "print \"\\nExample sentence: '{}'\".format(sentences[4])\n",
    "print \"\\nExample sentence after Pre-processing: '{}'\".format(token_sentences[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word2index[word] for word in sent[:-1]] for sent in token_sentences])\n",
    "y_train = np.asarray([[word2index[word] for word in sent[1:]] for sent in token_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ [0, 6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2],\n",
       "       [0, 11, 17, 7, 3114, 6036, 7999, 7999, 6036, 2]], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START i joined a new league this year and they have different scoring rules than i 'm used to . \n",
      "[0, 6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2]\n",
      "y:\n",
      "i joined a new league this year and they have different scoring rules than i 'm used to . SENTENCE_END \n",
      "[6, 3513, 7, 155, 794, 25, 223, 8, 32, 20, 202, 5025, 350, 91, 6, 66, 207, 5, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print \"x:\\n{} \\n{}\".format(\" \".join([index2vocab[index] for index in X_train[0]]), X_train[0])\n",
    "print \"y:\\n{} \\n{}\".format(\" \".join([index2vocab[index] for index in y_train[0]]), y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        xt = np.exp(x - np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # total number of time steps\n",
    "        T = len(x)\n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        for t in np.arange(T):\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = self.softmax(self.V.dot(s[t]))     \n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488\n",
      "   0.00012508]\n",
      " [ 0.00012536  0.00012582  0.00012436 ...,  0.00012482  0.00012456\n",
      "   0.00012451]\n",
      " [ 0.00012387  0.0001252   0.00012474 ...,  0.00012559  0.00012588\n",
      "   0.00012551]\n",
      " ..., \n",
      " [ 0.00012414  0.00012455  0.0001252  ...,  0.00012487  0.00012494\n",
      "   0.0001263 ]\n",
      " [ 0.0001252   0.00012393  0.00012509 ...,  0.00012407  0.00012578\n",
      "   0.00012502]\n",
      " [ 0.00012472  0.0001253   0.00012487 ...,  0.00012463  0.00012536\n",
      "   0.00012665]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "RNN_model = RNN(vocabulary_size)\n",
    "o, s = RNN_model.forward_propagation(X_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481  238 2539\n",
      "   21 6548  261 1780 2005 1810 5376 4146  477 7051 4832 4991  897 3485   21\n",
      " 7291 2007 6006  760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575]\n",
      "students shortly museum ruining background hunt madden wr chicken immoral hadith lighter rude questions achieve but sells making fill arguing purchase grows feat head lube winners downside states steal but researchers christian utilize fire domain resolution 10-15 genuinely magical worship Ð² branches memes node preferred\n"
     ]
    }
   ],
   "source": [
    "pred = RNN_model.predict(X_train[10])\n",
    "print pred.shape\n",
    "print pred\n",
    "print \" \".join([index2vocab[index] for index in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
